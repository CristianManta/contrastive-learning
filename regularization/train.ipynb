{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import time, datetime\n",
    "import os, shutil\n",
    "import yaml\n",
    "import ast, bisect\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.autograd import grad\n",
    "import torchnet as tnt\n",
    "\n",
    "import dataloader\n",
    "from dataloader import cutout\n",
    "from models.resnet import ResNet\n",
    "\n",
    "# -------------\n",
    "# Initial setup\n",
    "# -------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "# args = parser.parse_args()\n",
    "seed = None\n",
    "# CUDA info\n",
    "has_cuda = torch.cuda.is_available()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set random seed\n",
    "if seed is None:\n",
    "    seed = int(time.time())\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "datadir = '/home/math/oberman-lab/data/'\n",
    "test_batch_size = 3\n",
    "batch_size = 3\n",
    "args_cutout = 16\n",
    "args_model = 'ResNet50'\n",
    "\n",
    "workers = 4\n",
    "test_loader = getattr(dataloader, 'cifar10')(datadir,\n",
    "                                             mode='test', transform=False,\n",
    "                                             batch_size=test_batch_size,\n",
    "                                             num_workers=workers,\n",
    "                                             shuffle=False,\n",
    "                                             pin_memory=has_cuda)\n",
    "\n",
    "transforms = [cutout(args_cutout, channels=3)]\n",
    "train_loader = getattr(dataloader, 'cifar10')(datadir,\n",
    "                                              mode='train', transform=True,\n",
    "                                              batch_size=batch_size,\n",
    "                                              training_transforms=transforms,\n",
    "                                              num_workers=workers,\n",
    "                                              shuffle=True,\n",
    "                                              pin_memory=has_cuda,\n",
    "                                              drop_last=True)\n",
    "\n",
    "model = ResNet([3,4,6,3],base_channels=64, block='Bottleneck')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "decay = 5e-4\n",
    "momentum = 0.9\n",
    "lr_schedule = '[[0,1],[60,0.2],[120,0.04],[160,0.008]]'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "\n",
    "if has_cuda:\n",
    "    criterion = criterion.cuda(0)\n",
    "    train_criterion = train_criterion.cuda(0)\n",
    "    model = model.cuda(0)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=lr,\n",
    "                      weight_decay=decay,\n",
    "                      momentum=momentum,\n",
    "                      nesterov=False)\n",
    "\n",
    "\n",
    "def scheduler(optimizer, lr_schedule):\n",
    "    \"\"\"Return a hyperparmeter scheduler for the optimizer\"\"\"\n",
    "    lS = np.array(ast.literal_eval(lr_schedule))\n",
    "    llam = lambda e: float(lS[max(bisect.bisect_right(lS[:, 0], e) - 1, 0), 1])\n",
    "    lscheduler = LambdaLR(optimizer, llam)\n",
    "\n",
    "    return lscheduler\n",
    "\n",
    "\n",
    "schedule = scheduler(optimizer, lr_schedule)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "penalty = 0.0005\n",
    "\n",
    "\n",
    "ix = 0  # count of gradient steps\n",
    "\n",
    "tik = penalty\n",
    "\n",
    "regularizing = tik > 0\n",
    "\n",
    "h = 1e-2  # finite difference step size\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "norm = 'L2'\n",
    "fd_order = 'O2'\n",
    "log_interval = 100\n",
    "\n",
    "def train(epoch, ttot):\n",
    "    global ix\n",
    "\n",
    "    # Put the model in train mode (unfreeze batch norm parameters)\n",
    "    model.train()\n",
    "\n",
    "    # Run through the training data\n",
    "    if has_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    tepoch = time.perf_counter()\n",
    "\n",
    "\n",
    "    for batch_ix, (x, target) in enumerate(train_loader):\n",
    "\n",
    "        if has_cuda:\n",
    "            x = x.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if regularizing:\n",
    "            x.requires_grad_(True)\n",
    "\n",
    "        prediction = model(x)\n",
    "        lx = train_criterion(prediction, target)\n",
    "        loss = lx.mean()\n",
    "\n",
    "        # Compute finite difference approximation of directional derivative of grad loss wrt inputs\n",
    "        if regularizing:\n",
    "\n",
    "            dx = grad(loss, x, retain_graph=True)[0]\n",
    "            sh = dx.shape\n",
    "            print(f\"sh = {sh}\")\n",
    "            x.requires_grad_(False)\n",
    "\n",
    "            # v is the finite difference direction.\n",
    "            # For example, if norm=='L2', v is the gradient of the loss wrt inputs\n",
    "            v = dx.view(sh[0], -1)\n",
    "            print(f\"v :{v.shape}\")\n",
    "            Nb, Nd = v.shape\n",
    "            print(f\"Nb = {Nb}\")\n",
    "            print(f\"Nd = {Nd}\")\n",
    "\n",
    "            if norm == 'L2':\n",
    "                nv = v.norm(2, dim=-1, keepdim=True)\n",
    "                print(f\"nv: {nv.shape}\")\n",
    "                print(f\"nv = {nv}\")\n",
    "                nz = nv.view(-1) > 0\n",
    "                print(f\"nz: {nz.shape}\")\n",
    "                print(f\"nz = {nz}\")\n",
    "                v[nz] = v[nz].div(nv[nz])\n",
    "                print(f\"v: {v.shape}\")\n",
    "                print(f\"v = {v}\")\n",
    "\n",
    "            if norm == 'L1':\n",
    "                v = v.sign()\n",
    "                v = v / np.sqrt(Nd)\n",
    "            elif norm == 'Linf':\n",
    "                vmax, Jmax = v.abs().max(dim=-1)\n",
    "                sg = v.sign()\n",
    "                I = torch.arange(Nb, device=v.device)\n",
    "                sg = sg[I, Jmax]\n",
    "\n",
    "                v = torch.zeros_like(v)\n",
    "                I = I * Nd\n",
    "                Ix = Jmax + I\n",
    "                v.put_(Ix, sg)\n",
    "\n",
    "            v = v.view(sh)\n",
    "            print(f\"v: {v.shape}\")\n",
    "            xf = x + h * v\n",
    "            print(f\" x: {x.shape}\")\n",
    "            print(f\" h = {h}\")\n",
    "            print(f\"xf: {xf.shape}\")\n",
    "\n",
    "\n",
    "            mf = model(xf)\n",
    "            print(f\"mf = {mf}\")\n",
    "            print(f\"mf : {mf.shape}\")\n",
    "            lf = train_criterion(mf, target)\n",
    "            print(f\"lf = {lf}\")\n",
    "            print(f\"lf: {lf.shape}\")\n",
    "\n",
    "            if fd_order == 'O2':\n",
    "                xb = x - h * v\n",
    "                mb = model(xb)\n",
    "                lb = train_criterion(mb, target)\n",
    "                H = 2 * h\n",
    "            else:\n",
    "                H = h\n",
    "                lb = lx\n",
    "            dl = (lf - lb) / H\n",
    "            print(f\"dl = {dl}\")# This is the finite difference approximation\n",
    "            # of the directional derivative of the loss\n",
    "            print(f\"dl: {dl.shape}\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        tik_penalty = torch.tensor(np.nan)\n",
    "        dlmean = torch.tensor(np.nan)\n",
    "        dlmax = torch.tensor(np.nan)\n",
    "        if tik > 0:\n",
    "            dl2 = dl.pow(2)\n",
    "            tik_penalty = dl2.mean() / 2\n",
    "            loss = loss + tik * tik_penalty\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if np.isnan(loss.data.item()):\n",
    "            raise ValueError('model returned nan during training')\n",
    "\n",
    "        t = ttot + time.perf_counter() - tepoch\n",
    "        fmt = '{:.4f}'\n",
    "\n",
    "        if (batch_ix % log_interval == 0 and batch_ix > 0):\n",
    "            print('[%2d, %3d] penalized training loss: %.3g' %\n",
    "                  (epoch, batch_ix, loss.data.item()))\n",
    "        ix += 1\n",
    "\n",
    "    if has_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    return ttot + time.perf_counter() - tepoch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "def main():\n",
    "    pct_max = 90.\n",
    "    fail_count = fail_max = 5\n",
    "    time = 0.\n",
    "    pct0 = 100.\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # Update the learning rate\n",
    "        schedule.step()\n",
    "\n",
    "        time = train(e, time)\n",
    "\n",
    "\n",
    "\n",
    "        if fail_count < 1:\n",
    "            raise ValueError('Percent error has not decreased in %d epochs' % fail_max)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh = torch.Size([3, 3, 32, 32])\n",
      "v :torch.Size([3, 3072])\n",
      "Nb = 3\n",
      "Nd = 3072\n",
      "nv: torch.Size([3, 1])\n",
      "nv = tensor([[74.3334],\n",
      "        [73.5516],\n",
      "        [71.6138]], device='cuda:0')\n",
      "nz: torch.Size([3])\n",
      "nz = tensor([True, True, True], device='cuda:0')\n",
      "v: torch.Size([3, 3072])\n",
      "v = tensor([[-0.0009, -0.0042, -0.0024,  ..., -0.0008,  0.0006,  0.0022],\n",
      "        [-0.0148,  0.0036,  0.0119,  ...,  0.0048,  0.0025, -0.0003],\n",
      "        [ 0.0021,  0.0059, -0.0071,  ..., -0.0040,  0.0030, -0.0013]],\n",
      "       device='cuda:0')\n",
      "v: torch.Size([3, 3, 32, 32])\n",
      " x: torch.Size([3, 3, 32, 32])\n",
      " h = 0.01\n",
      "xf: torch.Size([3, 3, 32, 32])\n",
      "mf = tensor([[0.9512, 0.0000, 0.8275, 0.0000, 0.0000, 1.3871, 1.2602, 0.0000, 0.8439,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9332, 0.0000, 0.0000, 0.0000, 0.6963, 0.0000,\n",
      "         0.5227],\n",
      "        [0.3339, 1.2672, 0.6552, 0.4004, 1.4296, 0.0000, 0.0000, 0.7156, 0.5057,\n",
      "         0.8510]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "mf : torch.Size([3, 10])\n",
      "lf = tensor([2.1383, 2.5829, 2.6243], device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "lf: torch.Size([3])\n",
      "dl = tensor([ 16.3106,  51.6454, -12.0150], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "dl: torch.Size([3])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/campus/dragos.manta@MCGILL.CA/.conda/envs/simclr/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}